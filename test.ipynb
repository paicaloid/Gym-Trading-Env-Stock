{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from agent import Agent\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from agent import Agent\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import datetime\n",
    "import gym_trading_env\n",
    "from gym_trading_env.downloader import download\n",
    "from gym_trading_env.renderer import Renderer\n",
    "import matplotlib.pyplot as plt\n",
    "from src.stock_trading_env.single_stock_env import SingleStockTradingEnv\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "from torch import nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_ta as ta\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "df = pd.read_csv(\"examples/data/AAPL.csv\", parse_dates=[\"date\"], index_col= \"date\")\n",
    "df.sort_index(inplace= True)\n",
    "df.dropna(inplace= True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Generating features\n",
    "# WARNING : the column names need to contain keyword 'feature' !\n",
    "df[\"feature_close\"] = df[\"close\"].pct_change()\n",
    "df[\"feature_open\"] = df[\"open\"]/df[\"close\"]\n",
    "df[\"feature_high\"] = df[\"high\"]/df[\"close\"]\n",
    "df[\"feature_low\"] = df[\"low\"]/df[\"close\"]\n",
    "# df[\"feature_volume\"] = df[\"Volume USD\"] / df[\"Volume USD\"].rolling(7*24).max()\n",
    "\n",
    "rsi_values = ta.rsi(df[\"close\"]).values.reshape(-1, 1)\n",
    "scaler = MinMaxScaler()\n",
    "df[\"feature_rsi\"] = scaler.fit_transform(rsi_values)\n",
    "df = df['2020-01-01':]\n",
    "df.dropna(inplace= True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>feature_close</th>\n",
       "      <th>feature_open</th>\n",
       "      <th>feature_high</th>\n",
       "      <th>feature_low</th>\n",
       "      <th>feature_rsi</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020-01-02 21:30:00+07:00</th>\n",
       "      <td>74.0600</td>\n",
       "      <td>75.150000</td>\n",
       "      <td>73.7975</td>\n",
       "      <td>75.0875</td>\n",
       "      <td>135647456.0</td>\n",
       "      <td>0.022816</td>\n",
       "      <td>0.986316</td>\n",
       "      <td>1.000832</td>\n",
       "      <td>0.982820</td>\n",
       "      <td>0.909138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-03 21:30:00+07:00</th>\n",
       "      <td>74.2875</td>\n",
       "      <td>75.145000</td>\n",
       "      <td>74.1250</td>\n",
       "      <td>74.3575</td>\n",
       "      <td>146535512.0</td>\n",
       "      <td>-0.009722</td>\n",
       "      <td>0.999059</td>\n",
       "      <td>1.010591</td>\n",
       "      <td>0.996873</td>\n",
       "      <td>0.818799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-06 21:30:00+07:00</th>\n",
       "      <td>73.4475</td>\n",
       "      <td>74.990000</td>\n",
       "      <td>73.1875</td>\n",
       "      <td>74.9500</td>\n",
       "      <td>118578576.0</td>\n",
       "      <td>0.007968</td>\n",
       "      <td>0.979953</td>\n",
       "      <td>1.000534</td>\n",
       "      <td>0.976484</td>\n",
       "      <td>0.838232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-07 21:30:00+07:00</th>\n",
       "      <td>74.9600</td>\n",
       "      <td>75.225000</td>\n",
       "      <td>74.3700</td>\n",
       "      <td>74.5975</td>\n",
       "      <td>111510620.0</td>\n",
       "      <td>-0.004703</td>\n",
       "      <td>1.004859</td>\n",
       "      <td>1.008412</td>\n",
       "      <td>0.996950</td>\n",
       "      <td>0.795818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-08 21:30:00+07:00</th>\n",
       "      <td>74.2900</td>\n",
       "      <td>76.109975</td>\n",
       "      <td>74.2890</td>\n",
       "      <td>75.7975</td>\n",
       "      <td>132363784.0</td>\n",
       "      <td>0.016086</td>\n",
       "      <td>0.980111</td>\n",
       "      <td>1.004122</td>\n",
       "      <td>0.980098</td>\n",
       "      <td>0.836896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-25 20:30:00+07:00</th>\n",
       "      <td>193.3300</td>\n",
       "      <td>194.440000</td>\n",
       "      <td>192.9150</td>\n",
       "      <td>193.6200</td>\n",
       "      <td>37283201.0</td>\n",
       "      <td>0.004514</td>\n",
       "      <td>0.998502</td>\n",
       "      <td>1.004235</td>\n",
       "      <td>0.996359</td>\n",
       "      <td>0.622804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-26 20:30:00+07:00</th>\n",
       "      <td>193.6700</td>\n",
       "      <td>195.640000</td>\n",
       "      <td>193.3200</td>\n",
       "      <td>194.5000</td>\n",
       "      <td>47471868.0</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.995733</td>\n",
       "      <td>1.005861</td>\n",
       "      <td>0.993933</td>\n",
       "      <td>0.646923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-27 20:30:00+07:00</th>\n",
       "      <td>196.0200</td>\n",
       "      <td>197.200000</td>\n",
       "      <td>192.5500</td>\n",
       "      <td>193.2200</td>\n",
       "      <td>47460180.0</td>\n",
       "      <td>-0.006581</td>\n",
       "      <td>1.014491</td>\n",
       "      <td>1.020598</td>\n",
       "      <td>0.996532</td>\n",
       "      <td>0.586048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-28 20:30:00+07:00</th>\n",
       "      <td>194.6700</td>\n",
       "      <td>196.626000</td>\n",
       "      <td>194.1400</td>\n",
       "      <td>195.8300</td>\n",
       "      <td>48291443.0</td>\n",
       "      <td>0.013508</td>\n",
       "      <td>0.994076</td>\n",
       "      <td>1.004065</td>\n",
       "      <td>0.991370</td>\n",
       "      <td>0.657577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-31 20:30:00+07:00</th>\n",
       "      <td>196.0600</td>\n",
       "      <td>196.490000</td>\n",
       "      <td>195.2600</td>\n",
       "      <td>196.4500</td>\n",
       "      <td>38824113.0</td>\n",
       "      <td>0.003166</td>\n",
       "      <td>0.998015</td>\n",
       "      <td>1.000204</td>\n",
       "      <td>0.993942</td>\n",
       "      <td>0.672853</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>900 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               open        high       low     close  \\\n",
       "date                                                                  \n",
       "2020-01-02 21:30:00+07:00   74.0600   75.150000   73.7975   75.0875   \n",
       "2020-01-03 21:30:00+07:00   74.2875   75.145000   74.1250   74.3575   \n",
       "2020-01-06 21:30:00+07:00   73.4475   74.990000   73.1875   74.9500   \n",
       "2020-01-07 21:30:00+07:00   74.9600   75.225000   74.3700   74.5975   \n",
       "2020-01-08 21:30:00+07:00   74.2900   76.109975   74.2890   75.7975   \n",
       "...                             ...         ...       ...       ...   \n",
       "2023-07-25 20:30:00+07:00  193.3300  194.440000  192.9150  193.6200   \n",
       "2023-07-26 20:30:00+07:00  193.6700  195.640000  193.3200  194.5000   \n",
       "2023-07-27 20:30:00+07:00  196.0200  197.200000  192.5500  193.2200   \n",
       "2023-07-28 20:30:00+07:00  194.6700  196.626000  194.1400  195.8300   \n",
       "2023-07-31 20:30:00+07:00  196.0600  196.490000  195.2600  196.4500   \n",
       "\n",
       "                                volume  feature_close  feature_open  \\\n",
       "date                                                                  \n",
       "2020-01-02 21:30:00+07:00  135647456.0       0.022816      0.986316   \n",
       "2020-01-03 21:30:00+07:00  146535512.0      -0.009722      0.999059   \n",
       "2020-01-06 21:30:00+07:00  118578576.0       0.007968      0.979953   \n",
       "2020-01-07 21:30:00+07:00  111510620.0      -0.004703      1.004859   \n",
       "2020-01-08 21:30:00+07:00  132363784.0       0.016086      0.980111   \n",
       "...                                ...            ...           ...   \n",
       "2023-07-25 20:30:00+07:00   37283201.0       0.004514      0.998502   \n",
       "2023-07-26 20:30:00+07:00   47471868.0       0.004545      0.995733   \n",
       "2023-07-27 20:30:00+07:00   47460180.0      -0.006581      1.014491   \n",
       "2023-07-28 20:30:00+07:00   48291443.0       0.013508      0.994076   \n",
       "2023-07-31 20:30:00+07:00   38824113.0       0.003166      0.998015   \n",
       "\n",
       "                           feature_high  feature_low  feature_rsi  \n",
       "date                                                               \n",
       "2020-01-02 21:30:00+07:00      1.000832     0.982820     0.909138  \n",
       "2020-01-03 21:30:00+07:00      1.010591     0.996873     0.818799  \n",
       "2020-01-06 21:30:00+07:00      1.000534     0.976484     0.838232  \n",
       "2020-01-07 21:30:00+07:00      1.008412     0.996950     0.795818  \n",
       "2020-01-08 21:30:00+07:00      1.004122     0.980098     0.836896  \n",
       "...                                 ...          ...          ...  \n",
       "2023-07-25 20:30:00+07:00      1.004235     0.996359     0.622804  \n",
       "2023-07-26 20:30:00+07:00      1.005861     0.993933     0.646923  \n",
       "2023-07-27 20:30:00+07:00      1.020598     0.996532     0.586048  \n",
       "2023-07-28 20:30:00+07:00      1.004065     0.991370     0.657577  \n",
       "2023-07-31 20:30:00+07:00      1.000204     0.993942     0.672853  \n",
       "\n",
       "[900 rows x 10 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praew/miniconda3/envs/ppo/lib/python3.11/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.add_metric to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.add_metric` for environment variables or `env.get_wrapper_attr('add_metric')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "env = gym.make(\n",
    "        \"single-stock-v0\",\n",
    "        name=\"BTCUSD\",\n",
    "        df=df,\n",
    "        windows=1,\n",
    "        positions=[0,0.5,1],  # From -1 (=SHORT), to +1 (=LONG)\n",
    "        initial_position=0,  #Initial position\n",
    "        trading_fees=0,  # 0.01% per stock buy / sell\n",
    "        # borrow_interest_rate=0.0003/100,  #per timestep (= 1h here)\n",
    "        # reward_function=reward_function,\n",
    "        portfolio_initial_value=100000,  # in FIAT (here, USD)\n",
    "        max_episode_duration='max',\n",
    "        verbose=1\n",
    "    )\n",
    "env.add_metric(\"Reward\", lambda history: history[\"reward\"][-1])\n",
    "env.add_metric(\n",
    "    \"Portfolio Valuation\", lambda history: history[\"portfolio_valuation\"][-1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Tuple, Type, Union\n",
    "\n",
    "import gym\n",
    "import torch as th\n",
    "from torch import nn\n",
    "from stable_baselines3.common.policies import ActorCriticPolicy\n",
    "\n",
    "\n",
    "class Transpose(nn.Module):\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        return x.view(batch_size, -1)\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom network for policy and value function.\n",
    "    It receives as input the features extracted by the feature extractor.\n",
    "\n",
    "    :param feature_dim: dimension of the features extracted with the features_extractor (e.g. features from a CNN)\n",
    "    :param last_layer_dim_pi: (int) number of units for the last layer of the policy network\n",
    "    :param last_layer_dim_vf: (int) number of units for the last layer of the value network\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        timesteps: int = 12,\n",
    "        last_layer_dim_pi: int = 64,\n",
    "        last_layer_dim_vf: int = 64,\n",
    "    ):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "\n",
    "        # IMPORTANT:\n",
    "        # Save output dimensions, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Policy network\n",
    "        self.policy_net = nn.Sequential(\n",
    "            nn.Linear(last_layer_dim_pi*timesteps*feature_dim, last_layer_dim_pi*feature_dim), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_pi*feature_dim, last_layer_dim_pi), nn.Tanh()\n",
    "        )\n",
    "        # Value network\n",
    "        self.value_net = nn.Sequential(\n",
    "            nn.Linear(last_layer_dim_vf*timesteps*feature_dim, last_layer_dim_vf*feature_dim), nn.ReLU(),\n",
    "            nn.Linear(last_layer_dim_vf*feature_dim, last_layer_dim_vf), nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: (th.Tensor, th.Tensor) latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.policy_net(features), self.value_net(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)\n",
    "\n",
    "\n",
    "class CustomActorCriticPolicy(ActorCriticPolicy):\n",
    "    def __init__(\n",
    "        self,\n",
    "        observation_space: gym.spaces.Space,\n",
    "        action_space: gym.spaces.Space,\n",
    "        lr_schedule: Callable[[float], float],\n",
    "        net_arch: Optional[List[Union[int, Dict[str, List[int]]]]] = None,\n",
    "        activation_fn: Type[nn.Module] = nn.Tanh,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "\n",
    "        super(CustomActorCriticPolicy, self).__init__(\n",
    "            observation_space,\n",
    "            action_space,\n",
    "            lr_schedule,\n",
    "            net_arch,\n",
    "            activation_fn,\n",
    "            # Pass remaining arguments to base class\n",
    "            *args,\n",
    "            **kwargs,\n",
    "        )\n",
    "        # Disable orthogonal initialization\n",
    "        self.ortho_init = False\n",
    "\n",
    "    def _build_mlp_extractor(self) -> None:\n",
    "        self.mlp_extractor = CustomNetwork(\n",
    "        last_layer_dim_pi = 1,\n",
    "        last_layer_dim_vf = 1,\n",
    "        timesteps = self.observation_space.shape[1],\n",
    "        feature_dim=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[256, 256])\n",
    "model = PPO('MlpPolicy', env,policy_kwargs=policy_kwargs,verbose=0,n_epochs=5, tensorboard_log=\"./PPO_tensorboard/\")\n",
    "model_2 = PPO(CustomActorCriticPolicy, env, verbose=0)\n",
    "# model_3 = PPO(\"CnnPolicy\", env, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): CustomNetwork(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=1, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=1, out_features=1, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=1, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=1, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 161.63%   |   Portfolio Return : 348.66%   |   Reward : 0.00112844452345307   |   Portfolio Valuation : 448657.90000000014   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 121.03%   |   Reward : 0.00309037262027712   |   Portfolio Valuation : 221026.5374999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 150.38%   |   Reward : 0.0029759014620204427   |   Portfolio Valuation : 250380.46250000014   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 37.12%   |   Reward : 0.002212136693653432   |   Portfolio Valuation : 137123.20000000004   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 91.84%   |   Reward : 0.0   |   Portfolio Valuation : 191841.05000000013   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 45.74%   |   Reward : 0.0023493995711029413   |   Portfolio Valuation : 145740.17500000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 88.34%   |   Reward : 0.002967104824984842   |   Portfolio Valuation : 188341.24999999994   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 119.73%   |   Reward : 0.0031085758071145576   |   Portfolio Valuation : 219734.25000000026   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 138.52%   |   Reward : 0.0031240564542703584   |   Portfolio Valuation : 238524.10000000003   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 67.88%   |   Reward : 0.0018601441206669415   |   Portfolio Valuation : 167884.9875000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 93.56%   |   Reward : 0.0028870127849573244   |   Portfolio Valuation : 193558.4999999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 158.37%   |   Reward : 0.0024995190784730375   |   Portfolio Valuation : 258372.27500000029   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 77.93%   |   Reward : 0.0008771478471372028   |   Portfolio Valuation : 177927.16249999986   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 156.25%   |   Reward : 0.003150326915913329   |   Portfolio Valuation : 256249.67499999993   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 141.94%   |   Reward : 0.0030799049478347244   |   Portfolio Valuation : 241938.08749999985   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 99.86%   |   Reward : 0.001151470771139565   |   Portfolio Valuation : 199859.56249999997   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 85.98%   |   Reward : 0.003004862541684709   |   Portfolio Valuation : 185978.15000000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 151.28%   |   Reward : 0.0029652051273550565   |   Portfolio Valuation : 251282.3125000003   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 110.38%   |   Reward : 0.002951455846540706   |   Portfolio Valuation : 210375.97500000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 63.77%   |   Reward : 0.002078260523318822   |   Portfolio Valuation : 163768.41250000015   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 100.81%   |   Reward : 0.0011460404129864687   |   Portfolio Valuation : 200806.0250000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 131.51%   |   Reward : 0.0018548058123323552   |   Portfolio Valuation : 231505.62499999994   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 118.76%   |   Reward : 0.002049985102795629   |   Portfolio Valuation : 218762.25000000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 117.36%   |   Reward : 0.0025058900346468112   |   Portfolio Valuation : 217360.6499999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 162.25%   |   Reward : 0.0024624839365691036   |   Portfolio Valuation : 262253.275   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 117.84%   |   Reward : 0.0031356635868245533   |   Portfolio Valuation : 217838.99999999983   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 173.20%   |   Reward : 0.002363678632753321   |   Portfolio Valuation : 273202.36250000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 182.54%   |   Reward : 0.001934354358280308   |   Portfolio Valuation : 282537.81250000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 99.03%   |   Reward : 0.0005779764865420552   |   Portfolio Valuation : 199027.5375000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 124.71%   |   Reward : 0.0019956412122630034   |   Portfolio Valuation : 224713.32499999966   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 30.68%   |   Reward : 0.0014243287675310377   |   Portfolio Valuation : 130680.84999999995   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 130.63%   |   Reward : 0.0008458499286694478   |   Portfolio Valuation : 230634.86250000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 101.34%   |   Reward : 0.0015408922850016344   |   Portfolio Valuation : 201337.175   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 236.93%   |   Reward : 0.003133131252131742   |   Portfolio Valuation : 336931.9500000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 118.50%   |   Reward : 0.001965272469660393   |   Portfolio Valuation : 218504.91250000027   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 51.72%   |   Reward : 0.0028645811239566693   |   Portfolio Valuation : 151722.67500000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 93.36%   |   Reward : 0.002293708280546821   |   Portfolio Valuation : 193358.61250000013   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 203.16%   |   Reward : 0.0030724036764381655   |   Portfolio Valuation : 303159.8374999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 84.84%   |   Reward : 0.001965811233435257   |   Portfolio Valuation : 184838.15000000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 26.08%   |   Reward : 0.002406045811913338   |   Portfolio Valuation : 126084.32500000007   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 59.00%   |   Reward : 0.00254415026671746   |   Portfolio Valuation : 158997.73749999993   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 59.37%   |   Reward : 0.0021356220234679244   |   Portfolio Valuation : 159374.2625000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 109.77%   |   Reward : 0.002028044363657806   |   Portfolio Valuation : 209774.06250000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 133.82%   |   Reward : 0.002921031125078351   |   Portfolio Valuation : 233820.36249999993   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 102.10%   |   Reward : 0.0021051421890688695   |   Portfolio Valuation : 202099.17500000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 84.55%   |   Reward : 0.001903778822259155   |   Portfolio Valuation : 184545.71250000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 141.18%   |   Reward : 0.0030895937533213674   |   Portfolio Valuation : 241180.55   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 139.58%   |   Reward : 0.001955353809723354   |   Portfolio Valuation : 239576.94999999995   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 116.34%   |   Reward : 0.0011701614816599223   |   Portfolio Valuation : 216336.00000000017   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 166.36%   |   Reward : 0.0011231515080762948   |   Portfolio Valuation : 266364.725   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 96.91%   |   Reward : 0.003153601976775302   |   Portfolio Valuation : 196910.75000000017   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 141.37%   |   Reward : 0.0019407766666381698   |   Portfolio Valuation : 241374.65000000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 43.82%   |   Reward : 0.0030223240443351196   |   Portfolio Valuation : 143815.2125000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 63.14%   |   Reward : 0.0024794955762153167   |   Portfolio Valuation : 163138.44999999995   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 82.47%   |   Reward : 0.0019913399777380923   |   Portfolio Valuation : 182470.875   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 70.37%   |   Reward : 0.001832963936980355   |   Portfolio Valuation : 170372.16249999995   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 154.40%   |   Reward : 0.002928780087972759   |   Portfolio Valuation : 254402.86250000022   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 195.83%   |   Reward : 0.001979438265248223   |   Portfolio Valuation : 295830.98749999993   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 108.93%   |   Reward : 0.002971969643754597   |   Portfolio Valuation : 208926.01249999984   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 62.06%   |   Reward : 0.002100170854829888   |   Portfolio Valuation : 162061.6499999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 85.23%   |   Reward : 0.0030169417546316653   |   Portfolio Valuation : 185234.65000000005   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 71.04%   |   Reward : 0.0029041971247017436   |   Portfolio Valuation : 171035.425   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 50.57%   |   Reward : 0.0028864511561669556   |   Portfolio Valuation : 150574.74999999997   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 48.11%   |   Reward : 0.0012565991043195924   |   Portfolio Valuation : 148111.5874999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 159.54%   |   Reward : 0.0020557710142402215   |   Portfolio Valuation : 259536.7125000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 129.60%   |   Reward : 0.0029747964825964494   |   Portfolio Valuation : 229600.55   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 152.58%   |   Reward : 0.0029499682459077437   |   Portfolio Valuation : 252578.2874999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 104.67%   |   Reward : 0.0030339164715650646   |   Portfolio Valuation : 204666.475   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 69.46%   |   Reward : 0.0020083692219221953   |   Portfolio Valuation : 169461.63750000024   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 203.17%   |   Reward : 0.0030722927896724207   |   Portfolio Valuation : 303170.76249999984   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 112.87%   |   Reward : 0.0029168022252656863   |   Portfolio Valuation : 212871.7000000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 150.85%   |   Reward : 0.0029703595446035585   |   Portfolio Valuation : 250846.91250000003   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 106.59%   |   Reward : 0.001889608343272993   |   Portfolio Valuation : 206587.03749999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 145.67%   |   Reward : 0.002469803113473711   |   Portfolio Valuation : 245666.8125   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 119.57%   |   Reward : 0.0031109035657371476   |   Portfolio Valuation : 219570.08749999997   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 121.46%   |   Reward : 0.003084251395834309   |   Portfolio Valuation : 221464.5249999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 158.76%   |   Reward : 0.003119773478819797   |   Portfolio Valuation : 258755.30000000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 171.57%   |   Reward : 0.002972319441484568   |   Portfolio Valuation : 271571.8999999997   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 220.38%   |   Reward : 0.0019495956196223867   |   Portfolio Valuation : 320378.47499999974   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 78.67%   |   Reward : 0.0031279021110116947   |   Portfolio Valuation : 178673.47499999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 110.87%   |   Reward : 0.0018511899386163748   |   Portfolio Valuation : 210870.3625000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 167.20%   |   Reward : 0.003021047286894367   |   Portfolio Valuation : 267198.1000000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 69.71%   |   Reward : 0.0014623640005898004   |   Portfolio Valuation : 169712.4500000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 181.79%   |   Reward : 0.0030850389181255127   |   Portfolio Valuation : 281792.0999999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 141.73%   |   Reward : 0.003082574988613527   |   Portfolio Valuation : 241728.85000000027   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 97.46%   |   Reward : 0.00314478189035889   |   Portfolio Valuation : 197462.15000000014   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 53.85%   |   Reward : 0.002824918784800568   |   Portfolio Valuation : 153849.83749999988   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 68.54%   |   Reward : 0.0   |   Portfolio Valuation : 168543.51249999995   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 120.01%   |   Reward : 0.0031047322675294927   |   Portfolio Valuation : 220005.84999999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 141.26%   |   Reward : 0.0030885136759253077   |   Portfolio Valuation : 241264.7624999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 48.40%   |   Reward : 0.0029287705969637157   |   Portfolio Valuation : 148402.1500000002   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 62.22%   |   Reward : 0.003062185389755479   |   Portfolio Valuation : 162223.95   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 120.77%   |   Reward : 0.0030940011654962017   |   Portfolio Valuation : 220767.72499999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 127.57%   |   Reward : 0.0030013316724984427   |   Portfolio Valuation : 227573.63750000007   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 88.55%   |   Reward : 0.0023523156852615994   |   Portfolio Valuation : 188546.64999999994   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 152.07%   |   Reward : 0.0010955404022101028   |   Portfolio Valuation : 252068.48750000034   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 66.96%   |   Reward : 0.0029752380054241536   |   Portfolio Valuation : 166957.47500000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 290.00%   |   Reward : 0.0030251148498985454   |   Portfolio Valuation : 389995.9999999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 98.90%   |   Reward : 0.0031219881438068835   |   Portfolio Valuation : 198901.5625   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 165.21%   |   Reward : 0.003043745058254248   |   Portfolio Valuation : 265208.5624999998   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 111.11%   |   Reward : 0.0029411745794835965   |   Portfolio Valuation : 211110.2875000001   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 146.98%   |   Reward : 0.0030168788538403697   |   Portfolio Valuation : 246984.67500000008   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 137.44%   |   Reward : 0.0031383929360465574   |   Portfolio Valuation : 237436.20000000027   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 130.74%   |   Reward : 0.0029601422690156   |   Portfolio Valuation : 230735.50000000003   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 72.41%   |   Reward : 0.0028810495789087604   |   Portfolio Valuation : 172407.6   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 93.49%   |   Reward : 0.002292100052710104   |   Portfolio Valuation : 193494.12500000003   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 160.71%   |   Reward : 0.0019465579221553976   |   Portfolio Valuation : 260713.3374999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 188.30%   |   Reward : 0.0030153386397251745   |   Portfolio Valuation : 288295.75000000006   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 209.55%   |   Reward : 0.003008855213142721   |   Portfolio Valuation : 309552.88750000007   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 166.24%   |   Reward : 0.0030318812504664043   |   Portfolio Valuation : 266244.7499999999   |   \n",
      "Market Return : 161.63%   |   Portfolio Return : 97.90%   |   Reward : 0.00255501545960261   |   Portfolio Valuation : 197903.075   |   \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f914df34610>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (pi_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (vf_features_extractor): FlattenExtractor(\n",
       "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=5, out_features=256, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
       "      (3): ReLU()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=256, out_features=3, bias=True)\n",
       "  (value_net): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3.common.policies import obs_as_tensor\n",
    "\n",
    "def predict_proba(model, state):\n",
    "    obs = model.policy.obs_to_tensor(state)[0]\n",
    "    dis = model.policy.get_distribution(obs)\n",
    "    probs = dis.distribution.probs\n",
    "    probs_np = probs.detach().numpy()\n",
    "    return probs_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market Return : 161.63%   |   Portfolio Return : 158.81%   |   Reward : 0.0031190979910628498   |   Portfolio Valuation : 258811.25   |   \n",
      "0.9509288455547625\n"
     ]
    }
   ],
   "source": [
    "score_history_pred = []\n",
    "action_history = []\n",
    "model = model\n",
    "score_pred = 0\n",
    "reward_tot = 0\n",
    "######predict########\n",
    "obs, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "while not done and not truncated:\n",
    "    obs = obs[np.newaxis, ...]\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    action_history.append(action[0])\n",
    "    obs, reward, done, truncated, info = env.step(action[0])\n",
    "    reward_tot += reward\n",
    "    \n",
    "print(reward_tot)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluate(model, env, num_steps \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m, isrender\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluate' is not defined"
     ]
    }
   ],
   "source": [
    "evaluate(model, env, num_steps = 1000, isrender=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    }
   ],
   "source": [
    "print(action_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/praew/miniconda3/envs/ppo/lib/python3.11/site-packages/stable_baselines3/common/env_checker.py:238: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f0e9675ce50>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ppo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
